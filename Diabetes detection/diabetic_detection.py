# -*- coding: utf-8 -*-
"""Diabetic_detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mkDn_CFlTgHOTXcOAiGDFItNzbZsBSuF
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.metrics import categorical_crossentropy
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dense
from tensorflow.keras.optimizers import Adam
import pandas as pd
import numpy as np
import sklearn
from sklearn import svm
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder

data = pd.read_csv("/content/diabetes_prediction_dataset.csv")

data.head()

data.tail(10)

# Split the data into features (X) and labels (y)
X = data.drop('diabetes', axis=1)
Y = data['diabetes']

print(X)

print(Y)

# Create a StandardScaler object
scaler = StandardScaler()

X_scaled = scaler.fit_transform(X)
Y_scaled = scaler.fit_transform(Y)

# Create a LabelEncoder object
le = LabelEncoder()

# Convert the string values in the 'gender' and 'smoking_history' column to integers
X['gender'] = le.fit_transform(X['gender'])
X['smoking_history'] = le.fit_transform(X['smoking_history'])

# Standardize the data
X_scaled = scaler.fit_transform(X)

mean_values = scaler.mean_
scale_values = scaler.scale_

print("Mean Values:", mean_values)
print("Scale Values:", scale_values)

print(X_scaled)

print(Y)

"""# 0---> Non-diabetic

# 1---> diabetic

"""

import tensorflow as tf

# Define your neural network model with a name
model = tf.keras.Sequential(name="my_neural_network_model")
model.add(tf.keras.layers.Dense(units=16, input_shape=(X_scaled.shape[1],), activation='relu'))
model.add(tf.keras.layers.Dense(units=32, activation='relu'))
model.add(tf.keras.layers.Dense(units=2, activation='softmax'))

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Fit the model to the training data
model.fit(X_scaled, Y, epochs=10, batch_size=10, validation_split=0.2)

# Save the model to a file
model.save('my_neural_network_model.h5')

# Make predictions on the test set
predictions = model.predict(X_scaled, batch_size=15, verbose=2)

# Evaluate the model on the test data
loss, accuracy = model.evaluate(X_scaled, Y, verbose=2)

# Print the evaluation metrics
print("Test accuracy:", accuracy)
print("Test loss:", loss)

# Fit the model to the training data
history = model.fit(X_scaled, Y, epochs=10, batch_size=15, validation_split=0.2)

# Plot the training and validation accuracy and loss at each epoch
import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

from sklearn.metrics import f1_score, confusion_matrix

# Make predictions on the test data
y_pred = model.predict(X_scaled)

from sklearn.metrics import f1_score, confusion_matrix

# Calculate F1 score
f1 = f1_score(Y, Y, average='binary')
print("F1 score:", f1)

# Calculate confusion matrix
conf_matrix = confusion_matrix(Y, Y)
print("Confusion matrix:\n", conf_matrix)

import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from keras.models import load_model
from sklearn.model_selection import train_test_split

# Load the saved model
model_path = 'my_neural_network_model.h5'
model = load_model(model_path)

# Load the scaler used during training
scaler_mean = np.array([4.12489102e-01, 4.20056176e+01, 7.71941296e-02, 3.87968614e-02,
                         2.19925167e+00, 2.73698445e+01, 5.52441514e+00, 1.38018817e+02])
scaler_std = np.array([0.4926511, 22.4976091, 0.26689922, 0.1931105, 1.88568331, 6.68206636,
                          1.07879452, 40.99275629])

# Function to preprocess user input
def preprocess_input(input_query):
    gender, age, hypertension, heart_disease, smoking_history, bmi, HbA1c_level, blood_glucose_level = input_query

    # Handle unexpected input values
    gender = gender if gender in ['Female', 'Male'] else 'Female'
    smoking_history = smoking_history if smoking_history in ['never', 'former', 'No info', 'current'] else 'never'

    # Convert string values to integers using LabelEncoder
    gender_encoded = 0 if gender == 'Female' else 1
    smoking_history_encoded = {'never': 0, 'former': 1, 'No info': 2, 'current': 3}[smoking_history]

    # Convert input_query to array and standardize
    input_array = np.array([gender_encoded, age, hypertension, heart_disease, smoking_history_encoded, bmi, HbA1c_level, blood_glucose_level])
    input_scaled = (input_array - scaler_mean) / scaler_std

    return input_scaled

# Function to make prediction
def make_prediction(input_queries, threshold):
    preprocessed_inputs = np.array([preprocess_input(query) for query in input_queries])
    predictions = model.predict(preprocessed_inputs)
    labels = np.where(predictions > threshold, "Non-diabetic", "Diabetic")
    return labels

# Function to find optimal threshold using validation set
def find_optimal_threshold(X_val, Y_val):
    thresholds = np.linspace(0, 1, 100)  # Generate a range of threshold values
    best_accuracy = 0
    optimal_threshold = 0

    for threshold in thresholds:
        correct_predictions = 0
        total_predictions = len(X_val)

        # Make predictions for the entire validation set
        predictions = make_prediction(X_val, threshold)

        # Count correct predictions
        for pred, true_label in zip(predictions, Y_val):
            if ((pred == "Non-diabetic").all() and true_label == 0) or ((pred == "Diabetic").all() and true_label == 1):
                correct_predictions += 1

        # Calculate accuracy
        accuracy = correct_predictions / total_predictions

        # Update optimal threshold if accuracy improves
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            optimal_threshold = threshold

    return optimal_threshold

# Assuming X_scaled and Y are your features and labels
# Load your dataset (Replace the following lines with your actual data loading code)
X_scaled = np.random.rand(27528, 8)  # Replace this with your actual scaled dataset
Y = np.random.randint(0, 2, 27528)  # Replace this with your actual labels

# Find optimal threshold using validation set
optimal_threshold = find_optimal_threshold(X_scaled[0:1000], Y[0:1000])

# Example usage
input_query = ('Female', 20.0, 0, 0, 'never', 27.32, 6.6, 200)
predictions = make_prediction(np.array([input_query]), optimal_threshold)
print("Predictions with dynamic threshold:", predictions)

import time

start_time = time.time()
prediction = make_prediction(input_query, optimal_threshold)
end_time = time.time()

print("Prediction time:", end_time - start_time)